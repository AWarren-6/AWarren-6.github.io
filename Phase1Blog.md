# Cognitive Bias in Data Science: Potential Pitfalls for Future Scientists

  Data Science is a rapidly growing field that allows us to detect fraud, improve healthcare services, recommend items for customers, amongst many other things. Like any great technology, this has the potential to push us as a society to even new, never before seen areas. However, despite the advances we make in this area, there is one facet of data science that we need to keep in mind; As humans, we ourselves are prone to human errors and one of the more prominent ones that can affect us in our data science career is cognitive bias. In this blog, I will be reviewing several cognitive bias that we can encounter during our tenure.
    Before I proceed, let me explain that cognitive bias are mental deviations from what would be considered normal in regards to making decisions/general judgment. Think of cognitive bias as an outlier(s) of what is actually happening but our way of thinking is shifting that perspective slightly (or greatly if one isn't careful): it stands out vividly and does skew the results/data to unproduce undesired results. One such bias is called survivorship bias, which mostly focuses on events/individuals/etc. that have succeeded or survived through some event(s) and pretty much ignores the other failed entities due to the fact that they do not exist/have failed/are irrelevant, despite the fact that those "missed" events are still critical in understanding the bigger picture at hand.
    With that out of the way, let's get into those bias.   
    The first on the list is selection bias. Selection bias occurs when items chosen for sampling are not chosen at random, leading the results to skew towards some altered direction or at least not be representative of the population as a whole. An example of this is selecting only healthy volunteers to get data on for a healthcare-related directive/product while they are not statistically representative of the population as a whole, leading to misaligned products/services that only benefit a select few of the population.
     Next we have confirmation bias. One of the leading roles we as data scientists have, or just scientists in general, is to start our research with open minds with a look towards all types of data from all available sources. Confirmation bias, however, is the opposite of that. It is the act of searching for information/data/etc. that already confirms whatever hypothesis we were working towards answering in the first place. An example of this bias can be found in the political arena. If one has a strong personal belief on a particular politcal stance, one would be inclined to look towards data that would support this belief of theirs while simultaneously ignoring data from sources that could potentially disprove their stance/personal belief.
    Finally, we have interviewer bias. This mostly manifests in how we, as potential interviewers, frame our questions to achieve the outcome we want to hear. An example of this is, for asking about people's experience with some particular food item X, us asking the interviewee "How badly did eating item X affect your life recently". This is a leading question that's loaded with the answer the interviewer is looking for. A better question would be, "How has eating item X affect you and your lifestyle recently?" This avoids leading and lets the response be more open ended instead of pointing to a particular conclusion.
    In short, there are many other bias to be aware of, but these are three of the ones I found more relevant to our line of work. It is up to us to keep these bias in mind so that we may produce more accurate results and lead towards more quality in our work going forward. 
    With that said, let's go out there and do our best and I'll see you all next time.  
    El Psy Congroo
